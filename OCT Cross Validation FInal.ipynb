{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation of OCT and Comparison with CART, XGBoost and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes on Implementing the OCT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nNotes:\\n  \\nimport_data_uci() imports data from UCIMLREPO and organize it as a dataframe, it takes the id number of the dataset from UCIMLREPO as an argument. \\n\\ncross_valid_set(df = import_data_uci(id#), # of desired observations) splits the dataset into 4 training and testing sets; for large datasets of >5000 observations or >5 features, \\nit will be better to sample the dataset to smaller size by setting the second argument to the desired size, otherwise it will take a long time to run. This function will split the input df into 4 training and testing sets. \\n\\nOCT = oct(df, depth of tree, complexity parameter alpha, run time limit, warmstart for a, warmstart for b) is the fit function of the OCT model; where warmstart for a and b are obtained from CART: warmstart_cart(fitted CART model)).\\n\\nOCT.compute_accuracy(X,y) returns the accuracy of the model on the test set.\\n\\nOCT.plot_tree_structure() plots the tree structure of the model. However Graphviz must be installed and added to the system path.\\n\\nYou can also test out the OCT-H model:\\nOCTH = oct_h(df, depth of tree, complexity parameter alpha, run time limit) is the fit function of the OCT-H model; but note that at this stage the model takes warmstart of a as a matrix \\nand the warmstart generated from CART is not effective and not compatible with the model. Furthermore the run time for any dataset >200 observations is very long.\\n\\nOCTH.compute_accuracy(X,y) returns the accuracy of the model on the test set.\\n\\nOCTH.plot_tree_structure() plots the tree structure of the model. However Graphviz must be installed and added to the system path.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Notes:\n",
    "  \n",
    "import_data_uci() imports data from UCIMLREPO and organize it as a dataframe, it takes the id number of the dataset from UCIMLREPO as an argument. \n",
    "\n",
    "cross_valid_set(df = import_data_uci(id#), # of desired observations) splits the dataset into 4 training and testing sets; for large datasets of >5000 observations or >5 features, \n",
    "it will be better to sample the dataset to smaller size by setting the second argument to the desired size, otherwise it will take a long time to run. This function will split the input df into 4 training and testing sets. \n",
    "\n",
    "OCT = oct(df, depth of tree, complexity parameter alpha, run time limit, warmstart for a, warmstart for b) is the fit function of the OCT model; where warmstart for a and b are obtained from CART: warmstart_cart(fitted CART model)).\n",
    "\n",
    "OCT.compute_accuracy(X,y) returns the accuracy of the model on the test set.\n",
    "\n",
    "OCT.plot_tree_structure() plots the tree structure of the model. However Graphviz must be installed and added to the system path.\n",
    "\n",
    "You can also test out the OCT-H model:\n",
    "OCTH = oct_h(df, depth of tree, complexity parameter alpha, run time limit) is the fit function of the OCT-H model; but note that at this stage the model takes warmstart of a as a matrix \n",
    "and the warmstart generated from CART is not effective and not compatible with the model. Furthermore the run time for any dataset >200 observations is very long.\n",
    "\n",
    "OCTH.compute_accuracy(X,y) returns the accuracy of the model on the test set.\n",
    "\n",
    "OCTH.plot_tree_structure() plots the tree structure of the model. However Graphviz must be installed and added to the system path.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from gurobipy import *\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from ucimlrepo import fetch_ucirepo as fetc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from optimal_classification_tree1 import oct\n",
    "from optimal_classification_tree1 import oct_h\n",
    "from graphviz import Digraph\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Process Functions For Cross Validation\n",
    "(for Data Imported from UCIMLREPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations: 150\n",
      "number of features: 4\n",
      "number of classes: 3\n",
      "     sepal length  sepal width  petal length  petal width  target\n",
      "0        0.222222     0.625000      0.067797     0.041667       0\n",
      "1        0.166667     0.416667      0.067797     0.041667       0\n",
      "2        0.111111     0.500000      0.050847     0.041667       0\n",
      "3        0.083333     0.458333      0.084746     0.041667       0\n",
      "4        0.194444     0.666667      0.067797     0.041667       0\n",
      "..            ...          ...           ...          ...     ...\n",
      "145      0.666667     0.416667      0.711864     0.916667       2\n",
      "146      0.555556     0.208333      0.677966     0.750000       2\n",
      "147      0.611111     0.416667      0.711864     0.791667       2\n",
      "148      0.527778     0.583333      0.745763     0.916667       2\n",
      "149      0.444444     0.416667      0.694915     0.708333       2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def import_data_uci(data_id):\n",
    "    dt = fetc(id = data_id)\n",
    "    X = dt.data.features \n",
    "    y = dt.data.targets \n",
    "    df = pd.DataFrame(X, columns=dt.data.feature_names)\n",
    "    df = df.dropna()\n",
    "    target_name = 'target'\n",
    "    df[target_name] = y\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    #Change non numeric columns to numeric\n",
    "    column_names = df.columns.tolist()\n",
    "    non_numeric_cols = df.drop([target_name],axis=1).select_dtypes(exclude=[np.number]).columns\n",
    "    if not non_numeric_cols.empty:\n",
    "        for col in non_numeric_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    for col in column_names[:len(column_names)-1]:\n",
    "        df[col] = StandardScaler().fit_transform(df[[col]])\n",
    "    \n",
    "    for col in column_names[:len(column_names)-1]:\n",
    "        df[col] = MinMaxScaler().fit_transform(df[[col]])\n",
    "        \n",
    "    #Convert target column to numeric\n",
    "    le = LabelEncoder()\n",
    "    df[target_name] = le.fit_transform(df[target_name])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cross_valid_set(df,observations=None):\n",
    "    if observations is not None:\n",
    "        df = df.sample(n=observations, random_state=42)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
    "    folds = np.array_split(df, 4)\n",
    "    s1_in = np.concatenate([folds[i] for i in [0, 1, 2]], axis=0)\n",
    "    s1_out = folds[3]\n",
    "    s2_in = np.concatenate([folds[i] for i in [1, 2, 3]], axis=0)\n",
    "    s2_out = folds[0]\n",
    "    s3_in = np.concatenate([folds[i] for i in [0, 2, 3]], axis=0)\n",
    "    s3_out = folds[1]\n",
    "    s4_in = np.concatenate([folds[i] for i in [0, 1, 3]], axis=0)\n",
    "    s4_out = folds[2]\n",
    "\n",
    "    s1_in = pd.DataFrame(s1_in)\n",
    "    s1_out = pd.DataFrame(s1_out)\n",
    "    s2_in = pd.DataFrame(s2_in)\n",
    "    s2_out = pd.DataFrame(s2_out)\n",
    "    s3_in = pd.DataFrame(s3_in)\n",
    "    s3_out = pd.DataFrame(s3_out)\n",
    "    s4_in = pd.DataFrame(s4_in)\n",
    "    s4_out = pd.DataFrame(s4_out)\n",
    "    \n",
    "    s1_out = s1_out.reset_index(drop=True)\n",
    "    s2_out = s2_out.reset_index(drop=True)\n",
    "    s3_out = s3_out.reset_index(drop=True)\n",
    "    s4_out = s4_out.reset_index(drop=True)\n",
    "    \n",
    "    s1_in.columns = df.columns.tolist()\n",
    "    s1_out.columns = df.columns.tolist()\n",
    "    s2_in.columns = df.columns.tolist()\n",
    "    s2_out.columns = df.columns.tolist()\n",
    "    s3_in.columns = df.columns.tolist()\n",
    "    s3_out.columns = df.columns.tolist()\n",
    "    s4_in.columns = df.columns.tolist()\n",
    "    s4_out.columns = df.columns.tolist()\n",
    "\n",
    "    return s1_in,s1_out,s2_in,s2_out,s3_in,s3_out,s4_in,s4_out\n",
    "\n",
    "df = import_data_uci(53)\n",
    "print(f'number of observations: {len(df)}')\n",
    "print(f'number of features: {len(df.columns)-1}')\n",
    "print(f'number of classes: {len(df.target.unique())}')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data and Check Data Status (From TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\"c:\\\\Users\\\\zhuoq\\\\Downloads\\\\AMS515 Project\\\\Loan Application Classification matrix_train.txt\", delim_whitespace=True).drop([\\'const\\'], axis=1)\\ndf.rename(columns={\\'scenario_benchmark\\': \\'target\\'}, inplace=True)\\n\\ncolumn_names = df.columns.tolist()\\nnon_numeric_cols = df.drop([\\'target\\'],axis=1).select_dtypes(exclude=[np.number]).columns\\nif not non_numeric_cols.empty:\\n    for col in non_numeric_cols:\\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\\nfor col in column_names[:len(column_names)-1]:\\n    df[col] = StandardScaler().fit_transform(df[[col]])\\n\\n#normalize the data    \\nfor col in column_names[:len(column_names)-1]:\\n    df[col] = MinMaxScaler().fit_transform(df[[col]])\\n\\n#Convert target column to numeric\\nle = LabelEncoder()\\ndf[\\'target\\'] = le.fit_transform(df[\\'target\\'])\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = pd.read_csv(\"c:\\\\Users\\\\zhuoq\\\\Downloads\\\\AMS515 Project\\\\Loan Application Classification matrix_train.txt\", delim_whitespace=True).drop(['const'], axis=1)\n",
    "df.rename(columns={'scenario_benchmark': 'target'}, inplace=True)\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "non_numeric_cols = df.drop(['target'],axis=1).select_dtypes(exclude=[np.number]).columns\n",
    "if not non_numeric_cols.empty:\n",
    "    for col in non_numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "for col in column_names[:len(column_names)-1]:\n",
    "    df[col] = StandardScaler().fit_transform(df[[col]])\n",
    "\n",
    "#normalize the data    \n",
    "for col in column_names[:len(column_names)-1]:\n",
    "    df[col] = MinMaxScaler().fit_transform(df[[col]])\n",
    "\n",
    "#Convert target column to numeric\n",
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['target'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = cross_valid_set(df)\n",
    "df_train1 = cvs[0]\n",
    "df_test1 = cvs[1]\n",
    "\n",
    "df_train2 = cvs[2]\n",
    "df_test2 = cvs[3]\n",
    "\n",
    "df_train3 = cvs[4]\n",
    "df_test3 = cvs[5]\n",
    "\n",
    "df_train4 = cvs[6]\n",
    "df_test4 = cvs[7]\n",
    "\n",
    "#set 1\n",
    "x_train1 = df_train1.drop(['target'], axis=1)\n",
    "y_train1 = df_train1['target'].astype('int')\n",
    "x_test1 = df_test1.drop(['target'], axis=1)\n",
    "y_test1 = df_test1['target'].astype('int')\n",
    "\n",
    "#set 2\n",
    "x_train2 = df_train2.drop(['target'], axis=1)\n",
    "y_train2 = df_train2['target'].astype('int')\n",
    "x_test2 = df_test2.drop(['target'], axis=1)\n",
    "y_test2 = df_test2['target'].astype('int')\n",
    "\n",
    "#set 3\n",
    "x_train3 = df_train3.drop(['target'], axis=1)\n",
    "y_train3 = df_train3['target'].astype('int')\n",
    "x_test3 = df_test3.drop(['target'], axis=1)\n",
    "y_test3 = df_test3['target'].astype('int')\n",
    "\n",
    "#set 4\n",
    "x_train4 = df_train4.drop(['target'], axis=1)\n",
    "y_train4 = df_train4['target'].astype('int')\n",
    "x_test4 = df_test4.drop(['target'], axis=1)\n",
    "y_test4 = df_test4['target'].astype('int')\n",
    "\n",
    "#Depth\n",
    "D = 2\n",
    "#Number of Classes\n",
    "k = len(df.target.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predefined Functions for CART Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmstart_cart(cart):\n",
    "    feature = cart.tree_.feature\n",
    "    threshold = cart.tree_.threshold\n",
    "    feature_indices = []\n",
    "    threshold_indices = []\n",
    "    for i in range(len(feature)):\n",
    "        if feature[i] != -2:\n",
    "            feature_indices.append(int(feature[i]))\n",
    "            threshold_indices.append(float(threshold[i]))\n",
    "    return feature_indices, threshold_indices\n",
    "\n",
    "def cart_accuracy(x_train, y_train, x_test, y_test,cart_tree):\n",
    "    cart_tree.fit(x_train, y_train)\n",
    "    y_pred_gini = cart_tree.predict(x_test)\n",
    "    y_pred_gini_train = cart_tree.predict(x_train)\n",
    "    return accuracy_score(y_train, y_pred_gini_train),accuracy_score(y_test, y_pred_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART in-sample accuracy for set 1: 0.9646017699115044\n",
      "CART out-sample accuracy for set 1: 0.9459459459459459\n",
      "Set parameter TimeLimit to value 7200\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (win64 - Windows 11.0 (26100.2))\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-9400 CPU @ 2.90GHz, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 6 physical cores, 6 logical processors, using up to 6 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  7200\n",
      "\n",
      "Optimize a model with 1536 rows, 506 columns and 8272 nonzeros\n",
      "Model fingerprint: 0xb2766e98\n",
      "Variable types: 7 continuous, 499 integer (483 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-05, 1e+02]\n",
      "  Objective range  [5e-01, 3e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+02]\n",
      "Presolve removed 472 rows and 7 columns\n",
      "Presolve time: 0.03s\n",
      "Presolved: 1064 rows, 499 columns, 6444 nonzeros\n",
      "Variable types: 3 continuous, 496 integer (476 binary)\n",
      "\n",
      "Root relaxation: objective 1.500000e+00, 229 iterations, 0.01 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    1.50000    0   77          -    1.50000      -     -    0s\n",
      "H    0     0                     119.8809524    1.50000  98.7%     -    0s\n",
      "     0     0    1.50000    0  112  119.88095    1.50000  98.7%     -    0s\n",
      "     0     0    1.50000    0  104  119.88095    1.50000  98.7%     -    0s\n",
      "H    0     0                      55.3095238    1.50000  97.3%     -    0s\n",
      "     0     0    1.50000    0   19   55.30952    1.50000  97.3%     -    0s\n",
      "H    0     0                      23.0238095    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0  113   23.02381    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0  226   23.02381    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0   30   23.02381    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0   64   23.02381    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0  140   23.02381    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0  127   23.02381    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0   20   23.02381    1.50000  93.5%     -    0s\n",
      "     0     0    1.50000    0   20   23.02381    1.50000  93.5%     -    0s\n",
      "H    0     0                      14.9523810    1.50000  90.0%     -    0s\n",
      "     0     2    1.50000    0   14   14.95238    1.50000  90.0%     -    0s\n",
      "H  102   100                      12.2619048    1.50000  87.8%  70.0    0s\n",
      "  3508   789     cutoff   37        12.26190    1.50000  87.8%  75.8    5s\n",
      "\n",
      "Cutting planes:\n",
      "  Learned: 16\n",
      "  Cover: 57\n",
      "  Clique: 1\n",
      "  MIR: 1\n",
      "  Flow cover: 14\n",
      "  GUB cover: 96\n",
      "  Inf proof: 4\n",
      "\n",
      "Explored 6731 nodes (423139 simplex iterations) in 9.58 seconds (11.81 work units)\n",
      "Thread count was 6 (of 6 available processors)\n",
      "\n",
      "Solution count 5: 12.2619 14.9524 23.0238 ... 119.881\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.226190476190e+01, best bound 1.226190476190e+01, gap 0.0000%\n",
      "OCT in-sample accuracy for set 1: 0.9646017699115044\n",
      "OCT out-sample accuracy for set 1: 0.9459459459459459\n"
     ]
    }
   ],
   "source": [
    "#CART\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=D, random_state=42)\n",
    "\n",
    "cart_in_ac1 = cart_accuracy(x_train1,y_train1,x_test1,y_test1,clf)[0]\n",
    "cart_out_ac1 = cart_accuracy(x_train1,y_train1,x_test1,y_test1,clf)[1]\n",
    "print(f'CART in-sample accuracy for set 1: {cart_in_ac1}')\n",
    "print(f'CART out-sample accuracy for set 1: {cart_out_ac1}')\n",
    "\n",
    "clf.fit(x_train1, y_train1) #change this for testing!!!!!!!!\n",
    "\"\"\"\n",
    "Tree Structure: Warm start for OCT\n",
    "\"\"\"\n",
    "feature_indices = warmstart_cart(clf)[0]\n",
    "threshold_indices = warmstart_cart(clf)[1]\n",
    "\n",
    "# OCT \n",
    "OCT = oct(df_train1,D,0.5,7200)\n",
    "OCT_ac_in1 = OCT.compute_accuracy(x_train1,y_train1)\n",
    "OCT_ac_out1 = OCT.compute_accuracy(x_test1,y_test1)\n",
    "print(f'OCT in-sample accuracy for set 1: {OCT_ac_in1}')\n",
    "print(f'OCT out-sample accuracy for set 1: {OCT_ac_out1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART in-sample accuracy for set 2: 0.9464285714285714\n",
      "CART out-sample accuracy for set 2: 0.9736842105263158\n",
      "Set parameter TimeLimit to value 7200\n",
      "Set parameter Cuts to value 2\n",
      "Set parameter Heuristics to value 0.5\n",
      "Set parameter MIPFocus to value 1\n",
      "Set parameter Presolve to value 2\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (win64 - Windows 11.0 (26100.2))\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-9400 CPU @ 2.90GHz, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 6 physical cores, 6 logical processors, using up to 6 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  7200\n",
      "Heuristics  0.5\n",
      "MIPFocus  1\n",
      "Cuts  2\n",
      "Presolve  2\n",
      "\n",
      "Optimize a model with 1523 rows, 502 columns and 8208 nonzeros\n",
      "Model fingerprint: 0x23b12e77\n",
      "Variable types: 7 continuous, 495 integer (479 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-05, 1e+02]\n",
      "  Objective range  [5e-01, 3e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+02]\n",
      "\n",
      "Warning: Completing partial solution with 493 unfixed non-continuous variables out of 495\n",
      "User MIP start did not produce a new incumbent solution\n",
      "\n",
      "Presolve removed 468 rows and 7 columns\n",
      "Presolve time: 0.08s\n",
      "Presolved: 1055 rows, 495 columns, 7105 nonzeros\n",
      "Variable types: 3 continuous, 492 integer (472 binary)\n",
      "Found heuristic solution: objective 47.4487179\n",
      "Root relaxation presolve removed 11 rows and 0 columns\n",
      "Root relaxation presolved: 1044 rows, 506 columns, 7072 nonzeros\n",
      "\n",
      "\n",
      "Root relaxation: objective 1.500000e+00, 240 iterations, 0.03 seconds (0.01 work units)\n",
      "Another try with MIP start\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                      18.7307692    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   27   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0  124   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0  123   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   21   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   30   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   19   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   29   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   35   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   42   18.73077    1.50000  92.0%     -    0s\n",
      "     0     0    1.50000    0   35   18.73077    1.50000  92.0%     -    1s\n",
      "     0     2    1.50000    0   10   18.73077    1.50000  92.0%     -    1s\n",
      "  1471   561    1.50000    7  115   18.73077    1.50000  92.0%  67.5    5s\n",
      "  2939   847    5.32917   11  187   18.73077    1.50000  92.0%  82.2   10s\n",
      "  3902   888    1.70284   23  246   18.73077    1.50000  92.0%  84.2   15s\n",
      "  5208   528   11.34828   32   56   18.73077    4.37179  76.7%  85.7   20s\n",
      "\n",
      "Cutting planes:\n",
      "  Learned: 37\n",
      "  Gomory: 135\n",
      "  Lift-and-project: 1\n",
      "  Cover: 427\n",
      "  Implied bound: 4\n",
      "  MIR: 237\n",
      "  Mixing: 1\n",
      "  StrongCG: 24\n",
      "  Flow cover: 706\n",
      "  GUB cover: 687\n",
      "  Inf proof: 27\n",
      "  Zero half: 23\n",
      "  RLT: 7\n",
      "  Relax-and-lift: 40\n",
      "\n",
      "Explored 6819 nodes (586100 simplex iterations) in 24.94 seconds (29.56 work units)\n",
      "Thread count was 6 (of 6 available processors)\n",
      "\n",
      "Solution count 2: 18.7308 47.4487 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.873076923077e+01, best bound 1.873076923077e+01, gap 0.0000%\n",
      "OCT in-sample accuracy for set 2: 0.9464285714285714\n",
      "OCT out-sample accuracy for set 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "#CART\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=D, random_state=42)\n",
    "\n",
    "cart_in_ac2 = cart_accuracy(x_train2,y_train2,x_test2,y_test2,clf)[0]\n",
    "cart_out_ac2 = cart_accuracy(x_train2,y_train2,x_test2,y_test2,clf)[1]\n",
    "print(f'CART in-sample accuracy for set 2: {cart_in_ac2}')\n",
    "print(f'CART out-sample accuracy for set 2: {cart_out_ac2}')\n",
    "\n",
    "clf.fit(x_train2, y_train2)\n",
    "\n",
    "\"\"\"\n",
    "Tree Structure: Warm start for OCT\n",
    "\"\"\"\n",
    "feature_indices = warmstart_cart(clf)[0]\n",
    "threshold_indices = warmstart_cart(clf)[1]\n",
    "\n",
    "# OCT \n",
    "OCT = oct(df_train2,D,0.5,7200,feature_indices,threshold_indices,2,0.5,1,2)\n",
    "OCT_ac_in2 = OCT.compute_accuracy(x_train2,y_train2)\n",
    "OCT_ac_out2 = OCT.compute_accuracy(x_test2,y_test2)\n",
    "print(f'OCT in-sample accuracy for set 2: {OCT_ac_in2}')\n",
    "print(f'OCT out-sample accuracy for set 2: {OCT_ac_out2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Set 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART in-sample accuracy for set 3: 0.9553571428571429\n",
      "CART out-sample accuracy for set 3: 0.9736842105263158\n",
      "Set parameter TimeLimit to value 7200\n",
      "Set parameter Cuts to value 2\n",
      "Set parameter Heuristics to value 0.5\n",
      "Set parameter MIPFocus to value 1\n",
      "Set parameter Presolve to value 2\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (win64 - Windows 11.0 (26100.2))\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-9400 CPU @ 2.90GHz, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 6 physical cores, 6 logical processors, using up to 6 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  7200\n",
      "Heuristics  0.5\n",
      "MIPFocus  1\n",
      "Cuts  2\n",
      "Presolve  2\n",
      "\n",
      "Optimize a model with 1523 rows, 502 columns and 8204 nonzeros\n",
      "Model fingerprint: 0x94193814\n",
      "Variable types: 7 continuous, 495 integer (479 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-05, 1e+02]\n",
      "  Objective range  [5e-01, 3e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+02]\n",
      "\n",
      "Warning: Completing partial solution with 493 unfixed non-continuous variables out of 495\n",
      "User MIP start did not produce a new incumbent solution\n",
      "\n",
      "Presolve removed 468 rows and 7 columns\n",
      "Presolve time: 0.05s\n",
      "Presolved: 1055 rows, 495 columns, 7094 nonzeros\n",
      "Variable types: 3 continuous, 492 integer (472 binary)\n",
      "Found heuristic solution: objective 53.1923077\n",
      "Root relaxation presolve removed 10 rows and 0 columns\n",
      "Root relaxation presolved: 1045 rows, 505 columns, 7064 nonzeros\n",
      "\n",
      "\n",
      "Root relaxation: objective 1.500000e+00, 276 iterations, 0.01 seconds (0.01 work units)\n",
      "Another try with MIP start\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    1.50000    0   47   53.19231    1.50000  97.2%     -    0s\n",
      "     0     0    1.50000    0   74   53.19231    1.50000  97.2%     -    0s\n",
      "     0     0    1.50000    0   73   53.19231    1.50000  97.2%     -    0s\n",
      "     0     0    1.50000    0   56   53.19231    1.50000  97.2%     -    0s\n",
      "     0     0    1.50000    0   56   53.19231    1.50000  97.2%     -    0s\n",
      "H    0     0                      18.7307692    1.50000  92.0%     -    0s\n",
      "H    0     0                      15.8589744    1.50000  90.5%     -    0s\n",
      "     0     0    1.50000    0   21   15.85897    1.50000  90.5%     -    0s\n",
      "     0     0    1.50000    0   30   15.85897    1.50000  90.5%     -    0s\n",
      "     0     0    1.57008    0   73   15.85897    1.57008  90.1%     -    0s\n",
      "     0     0    1.66617    0   94   15.85897    1.66617  89.5%     -    0s\n",
      "     0     0    4.37179    0   49   15.85897    4.37179  72.4%     -    0s\n",
      "     0     2    4.37179    0   29   15.85897    4.37179  72.4%     -    0s\n",
      "  2263   590    4.37179   24  143   15.85897    4.37179  72.4%  45.2    5s\n",
      "  3316   771    6.23755   24   42   15.85897    4.37179  72.4%  57.9   10s\n",
      "\n",
      "Cutting planes:\n",
      "  Learned: 9\n",
      "  Gomory: 44\n",
      "  Lift-and-project: 1\n",
      "  Cover: 158\n",
      "  Implied bound: 1\n",
      "  Clique: 1\n",
      "  MIR: 84\n",
      "  Mixing: 2\n",
      "  StrongCG: 9\n",
      "  Flow cover: 306\n",
      "  GUB cover: 340\n",
      "  Inf proof: 4\n",
      "  Zero half: 9\n",
      "  RLT: 4\n",
      "  Relax-and-lift: 16\n",
      "\n",
      "Explored 4268 nodes (274347 simplex iterations) in 13.77 seconds (15.63 work units)\n",
      "Thread count was 6 (of 6 available processors)\n",
      "\n",
      "Solution count 3: 15.859 18.7308 53.1923 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.585897435897e+01, best bound 1.585897435897e+01, gap 0.0000%\n",
      "OCT in-sample accuracy for set 3: 0.9553571428571429\n",
      "OCT out-sample accuracy for set 3: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "#CART\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=D, random_state=42)\n",
    "\n",
    "cart_in_ac3 = cart_accuracy(x_train3,y_train3,x_test3,y_test3,clf)[0]\n",
    "cart_out_ac3 = cart_accuracy(x_train3,y_train3,x_test3,y_test3,clf)[1]\n",
    "print(f'CART in-sample accuracy for set 3: {cart_in_ac3}')\n",
    "print(f'CART out-sample accuracy for set 3: {cart_out_ac3}')\n",
    "\n",
    "clf.fit(x_train3, y_train3)\n",
    "\n",
    "\"\"\"\n",
    "Tree Structure: Warm start for OCT\n",
    "\"\"\"\n",
    "feature_indices = warmstart_cart(clf)[0]\n",
    "threshold_indices = warmstart_cart(clf)[1]\n",
    "\n",
    "# OCT \n",
    "OCT = oct(df_train3,D,0.5,7200,feature_indices,threshold_indices,2,0.5,1,2)\n",
    "OCT_ac_in3 = OCT.compute_accuracy(x_train3,y_train3)\n",
    "OCT_ac_out3 = OCT.compute_accuracy(x_test3,y_test3)\n",
    "print(f'OCT in-sample accuracy for set 3: {OCT_ac_in3}')\n",
    "print(f'OCT out-sample accuracy for set 3: {OCT_ac_out3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Set 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART in-sample accuracy for set 4: 0.9823008849557522\n",
      "CART out-sample accuracy for set 4: 0.8918918918918919\n",
      "Set parameter TimeLimit to value 7200\n",
      "Set parameter Cuts to value 2\n",
      "Set parameter Heuristics to value 0.5\n",
      "Set parameter MIPFocus to value 1\n",
      "Set parameter Presolve to value 2\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (win64 - Windows 11.0 (26100.2))\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-9400 CPU @ 2.90GHz, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 6 physical cores, 6 logical processors, using up to 6 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  7200\n",
      "Heuristics  0.5\n",
      "MIPFocus  1\n",
      "Cuts  2\n",
      "Presolve  2\n",
      "\n",
      "Optimize a model with 1536 rows, 506 columns and 8280 nonzeros\n",
      "Model fingerprint: 0xfa432f02\n",
      "Variable types: 7 continuous, 499 integer (483 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-05, 1e+02]\n",
      "  Objective range  [5e-01, 3e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+02]\n",
      "\n",
      "Warning: Completing partial solution with 497 unfixed non-continuous variables out of 499\n",
      "User MIP start did not produce a new incumbent solution\n",
      "\n",
      "Presolve removed 472 rows and 7 columns\n",
      "Presolve time: 0.04s\n",
      "Presolved: 1064 rows, 499 columns, 7142 nonzeros\n",
      "Variable types: 3 continuous, 496 integer (476 binary)\n",
      "Found heuristic solution: objective 46.1052632\n",
      "Root relaxation presolve removed 11 rows and 0 columns\n",
      "Root relaxation presolved: 1053 rows, 510 columns, 7109 nonzeros\n",
      "\n",
      "\n",
      "Root relaxation: objective 1.500000e+00, 266 iterations, 0.01 seconds (0.01 work units)\n",
      "Another try with MIP start\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    1.50000    0   18   46.10526    1.50000  96.7%     -    0s\n",
      "H    0     0                      40.1578947    1.50000  96.3%     -    0s\n",
      "     0     0    1.50000    0  268   40.15789    1.50000  96.3%     -    0s\n",
      "H    0     0                      34.2105263    1.50000  95.6%     -    0s\n",
      "     0     0    1.50000    0  266   34.21053    1.50000  95.6%     -    0s\n",
      "     0     0    1.50000    0   23   34.21053    1.50000  95.6%     -    0s\n",
      "     0     0    1.50000    0   61   34.21053    1.50000  95.6%     -    0s\n",
      "     0     0    1.50000    0    2   34.21053    1.50000  95.6%     -    0s\n",
      "H    0     0                       7.4473684    1.50000  79.9%     -    0s\n",
      "     0     0    1.50000    0    8    7.44737    1.50000  79.9%     -    0s\n",
      "     0     0    1.50000    0    8    7.44737    1.50000  79.9%     -    0s\n",
      "     0     0    1.50000    0   17    7.44737    1.50000  79.9%     -    1s\n",
      "     0     2    1.50000    0   17    7.44737    1.50000  79.9%     -    1s\n",
      "  1333   278 infeasible   10         7.44737    1.50000  79.9%  75.9    5s\n",
      "  3011     0 infeasible   13         7.44737    4.47368  39.9%  75.8   10s\n",
      "\n",
      "Cutting planes:\n",
      "  Learned: 10\n",
      "  Gomory: 26\n",
      "  Cover: 269\n",
      "  Implied bound: 4\n",
      "  MIR: 469\n",
      "  StrongCG: 114\n",
      "  Flow cover: 1719\n",
      "  GUB cover: 262\n",
      "  Inf proof: 11\n",
      "  Zero half: 72\n",
      "  RLT: 35\n",
      "  Relax-and-lift: 116\n",
      "\n",
      "Explored 3053 nodes (235369 simplex iterations) in 10.11 seconds (12.71 work units)\n",
      "Thread count was 6 (of 6 available processors)\n",
      "\n",
      "Solution count 4: 7.44737 34.2105 40.1579 46.1053 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 7.447368421053e+00, best bound 7.447368421053e+00, gap 0.0000%\n",
      "OCT in-sample accuracy for set 4: 0.9823008849557522\n",
      "OCT out-sample accuracy for set 4: 0.8648648648648649\n"
     ]
    }
   ],
   "source": [
    "#CART\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=D, random_state=42)\n",
    "\n",
    "cart_ac = cart_accuracy(x_train4,y_train4,x_test4,y_test4,clf)\n",
    "cart_in_ac4 = cart_ac[0]\n",
    "cart_out_ac4 = cart_ac[1]\n",
    "print(f'CART in-sample accuracy for set 4: {cart_in_ac4}')\n",
    "print(f'CART out-sample accuracy for set 4: {cart_out_ac4}')\n",
    "\n",
    "clf.fit(x_train4, y_train4) #change this for testing!!!!!!!!\n",
    "\n",
    "\"\"\"\n",
    "Tree Structure: Warm start for OCT\n",
    "\"\"\"\n",
    "feature_indices = warmstart_cart(clf)[0]\n",
    "threshold_indices = warmstart_cart(clf)[1]\n",
    "\n",
    "# OCT \n",
    "OCT = oct(df_train4,D,0.5,7200,feature_indices,threshold_indices,2,0.5,1,2)\n",
    "OCT_ac_in4 = OCT.compute_accuracy(x_train4,y_train4)\n",
    "OCT_ac_out4 = OCT.compute_accuracy(x_test4,y_test4)\n",
    "print(f'OCT in-sample accuracy for set 4: {OCT_ac_in4}')\n",
    "print(f'OCT out-sample accuracy for set 4: {OCT_ac_out4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost in-sample accuracy for set 1: 1.0\n",
      "XGBoost out-sample accuracy for set 1: 0.9459459459459459\n",
      "XGBoost in-sample accuracy for set 2: 1.0\n",
      "XGBoost out-sample accuracy for set 2: 1.0\n",
      "XGBoost in-sample accuracy for set 3: 1.0\n",
      "XGBoost out-sample accuracy for set 3: 0.9473684210526315\n",
      "XGBoost in-sample accuracy for set 4: 1.0\n",
      "XGBoost out-sample accuracy for set 4: 0.8918918918918919\n"
     ]
    }
   ],
   "source": [
    "xgboost_in_ac = []\n",
    "xgboost_out_ac = []\n",
    "for i in range(1, 5):  \n",
    "    x_train = eval(f'x_train{i}')\n",
    "    y_train = eval(f'y_train{i}')\n",
    "    x_test = eval(f'x_test{i}')\n",
    "    y_test = eval(f'y_test{i}')\n",
    "    \n",
    "    if k >= 3:\n",
    "        model = xgb.XGBClassifier(objective='multi:softmax',num_class=k,use_label_encoder=False, max_depth=D)\n",
    "    else:\n",
    "        model = xgb.XGBClassifier(objective='binary:logistic',use_label_encoder=False, max_depth=D)\n",
    "        \n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_fit = model.predict(x_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    acc_train = accuracy_score(y_train, y_fit)\n",
    "    acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    xgboost_in_ac.append(acc_train)\n",
    "    xgboost_out_ac.append(acc_test)\n",
    "    \n",
    "    print(f'XGBoost in-sample accuracy for set {i}: {acc_train}')\n",
    "    print(f'XGBoost out-sample accuracy for set {i}: {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset 1 ===\n",
      "Random Forest Train Accuracy: 0.9734513274336283\n",
      "Random Forest Test Accuracy: 0.9459459459459459\n",
      "\n",
      "=== Dataset 2 ===\n",
      "Random Forest Train Accuracy: 0.9464285714285714\n",
      "Random Forest Test Accuracy: 1.0\n",
      "\n",
      "=== Dataset 3 ===\n",
      "Random Forest Train Accuracy: 0.9553571428571429\n",
      "Random Forest Test Accuracy: 0.9736842105263158\n",
      "\n",
      "=== Dataset 4 ===\n",
      "Random Forest Train Accuracy: 0.9823008849557522\n",
      "Random Forest Test Accuracy: 0.8648648648648649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_in_ac = []\n",
    "rf_out_ac = []\n",
    "for i in range(1, 5): \n",
    "    x_train = eval(f'x_train{i}')\n",
    "    y_train = eval(f'y_train{i}')\n",
    "    x_test = eval(f'x_test{i}')\n",
    "    y_test = eval(f'y_test{i}')\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42,max_depth=D) #build 100 decision trees\n",
    "    rf.fit(x_train, y_train)\n",
    "\n",
    "    y_fit = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "\n",
    "    acc_train = accuracy_score(y_train, y_fit)\n",
    "    acc_test = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    rf_in_ac.append(acc_train)\n",
    "    rf_out_ac.append(acc_test)\n",
    "\n",
    "    print(f\"=== Dataset {i} ===\")\n",
    "    print(\"Random Forest Train Accuracy:\", acc_train)\n",
    "    print(\"Random Forest Test Accuracy:\", acc_test)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCT-H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom optimal_classification_tree1 import oct_h\\nOCTH1 = oct_h(df_train1,D,0.5,7200)\\nocth_in_acc1 = OCTH1.compute_accuracy(x_train1,y_train1)\\nocth_out_acc1 = OCTH1.compute_accuracy(x_test1,y_test1)\\n\\nOCTH2 = oct_h(df_train2,D,0.5,7200)\\nocth_in_acc2 = OCTH2.compute_accuracy(x_train2,y_train2)\\nocth_out_acc2 = OCTH2.compute_accuracy(x_test2,y_test2)\\n\\nOCTH3 = oct_h(df_train3,D,0.5,7200)\\nocth_in_acc3 = OCTH3.compute_accuracy(x_train3,y_train3)\\nocth_out_acc3 = OCTH3.compute_accuracy(x_test3,y_test3)\\n\\nOCTH4 = oct_h(df_train4,D,0.5,7200)\\nocth_in_acc4 = OCTH4.compute_accuracy(x_train4,y_train4)\\nocth_out_acc4 = OCTH4.compute_accuracy(x_test4,y_test4)\\n\\n\\nprint(f'OCT-H in-sample accuracy for set 1: {octh_in_acc1}')\\nprint(f'OCT-H out-sample accuracy for set 1: {octh_out_acc1}')\\nprint(f'OCT-H in-sample accuracy for set 2: {octh_in_acc2}')\\nprint(f'OCT-H out-sample accuracy for set 2: {octh_out_acc2}')\\nprint(f'OCT-H in-sample accuracy for set 3: {octh_in_acc3}')\\nprint(f'OCT-H out-sample accuracy for set 3: {octh_out_acc3}')\\nprint(f'OCT-H in-sample accuracy for set 4: {octh_in_acc4}')\\nprint(f'OCT-H out-sample accuracy for set 4: {octh_out_acc4}')\\n\\nprint(f'Average in-sample accuracy for OCT-H: {(octh_in_acc1+octh_in_acc2+octh_in_acc3+octh_in_acc4)/4}')\\nprint(f'Average out-sample accuracy for OCT-H: {(octh_out_acc1+octh_out_acc2+octh_out_acc3+octh_out_acc4)/4}')\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from optimal_classification_tree1 import oct_h\n",
    "OCTH1 = oct_h(df_train1,D,0.5,7200)\n",
    "octh_in_acc1 = OCTH1.compute_accuracy(x_train1,y_train1)\n",
    "octh_out_acc1 = OCTH1.compute_accuracy(x_test1,y_test1)\n",
    "\n",
    "OCTH2 = oct_h(df_train2,D,0.5,7200)\n",
    "octh_in_acc2 = OCTH2.compute_accuracy(x_train2,y_train2)\n",
    "octh_out_acc2 = OCTH2.compute_accuracy(x_test2,y_test2)\n",
    "\n",
    "OCTH3 = oct_h(df_train3,D,0.5,7200)\n",
    "octh_in_acc3 = OCTH3.compute_accuracy(x_train3,y_train3)\n",
    "octh_out_acc3 = OCTH3.compute_accuracy(x_test3,y_test3)\n",
    "\n",
    "OCTH4 = oct_h(df_train4,D,0.5,7200)\n",
    "octh_in_acc4 = OCTH4.compute_accuracy(x_train4,y_train4)\n",
    "octh_out_acc4 = OCTH4.compute_accuracy(x_test4,y_test4)\n",
    "\n",
    "\n",
    "print(f'OCT-H in-sample accuracy for set 1: {octh_in_acc1}')\n",
    "print(f'OCT-H out-sample accuracy for set 1: {octh_out_acc1}')\n",
    "print(f'OCT-H in-sample accuracy for set 2: {octh_in_acc2}')\n",
    "print(f'OCT-H out-sample accuracy for set 2: {octh_out_acc2}')\n",
    "print(f'OCT-H in-sample accuracy for set 3: {octh_in_acc3}')\n",
    "print(f'OCT-H out-sample accuracy for set 3: {octh_out_acc3}')\n",
    "print(f'OCT-H in-sample accuracy for set 4: {octh_in_acc4}')\n",
    "print(f'OCT-H out-sample accuracy for set 4: {octh_out_acc4}')\n",
    "\n",
    "print(f'Average in-sample accuracy for OCT-H: {(octh_in_acc1+octh_in_acc2+octh_in_acc3+octh_in_acc4)/4}')\n",
    "print(f'Average out-sample accuracy for OCT-H: {(octh_out_acc1+octh_out_acc2+octh_out_acc3+octh_out_acc4)/4}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average In-Sample and Out-Sample Accuracy for CART, OCT, XGBoost, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average in-sample accuracy for CART: 0.9621720922882427\n",
      "Average out-sample accuracy for CART: 0.9463015647226174\n",
      "Average in-sample accuracy for OCT: 0.9621720922882427\n",
      "Average out-sample accuracy for OCT: 0.9461237553342817\n",
      "Average in-sample accuracy for XGBoost: 1.0\n",
      "Average out-sample accuracy for XGBoost: 0.9463015647226174\n",
      "Average in-sample accuracy for Random Forest: 0.9643844816687737\n",
      "Average out-sample accuracy for Random Forest: 0.9461237553342817\n"
     ]
    }
   ],
   "source": [
    "avg_in_ac_CART = (cart_in_ac1 + cart_in_ac2 + cart_in_ac3 + cart_in_ac4) / 4\n",
    "avg_out_ac_CART = (cart_out_ac1 + cart_out_ac2 + cart_out_ac3 + cart_out_ac4) / 4\n",
    "\n",
    "avg_in_ac_OCT = (OCT_ac_in1 + OCT_ac_in2 + OCT_ac_in3 + OCT_ac_in4) / 4\n",
    "avg_out_ac_OCT = (OCT_ac_out1 + OCT_ac_out2 + OCT_ac_out3 + OCT_ac_out4) / 4\n",
    "'''\n",
    "avg_in_ac_OCTH = (octh_in_acc1 + octh_in_acc2 + octh_in_acc3 + octh_in_acc4) / 4\n",
    "avg_out_ac_OCTH = (octh_out_acc1 + octh_out_acc2 + octh_out_acc3 + octh_out_acc4) / 4\n",
    "'''\n",
    "avg_in_ac_XGBoost = (xgboost_in_ac[0] + xgboost_in_ac[1] + xgboost_in_ac[2] + xgboost_in_ac[3]) / 4\n",
    "avg_out_ac_XGBoost = (xgboost_out_ac[0] + xgboost_out_ac[1] + xgboost_out_ac[2] + xgboost_out_ac[3]) / 4\n",
    "\n",
    "avg_in_ac_RF = (rf_in_ac[0] + rf_in_ac[1] + rf_in_ac[2] + rf_in_ac[3]) / 4\n",
    "avg_out_ac_RF = (rf_out_ac[0] + rf_out_ac[1] + rf_out_ac[2] + rf_out_ac[3]) / 4\n",
    "\n",
    "print(f'Average in-sample accuracy for CART: {avg_in_ac_CART}')\n",
    "print(f'Average out-sample accuracy for CART: {avg_out_ac_CART}')\n",
    "\n",
    "print(f'Average in-sample accuracy for OCT: {avg_in_ac_OCT}')\n",
    "print(f'Average out-sample accuracy for OCT: {avg_out_ac_OCT}')\n",
    "'''\n",
    "print(f'Average in-sample accuracy for OCTH: {avg_in_ac_OCTH}')\n",
    "print(f'Average out-sample accuracy for OCTH: {avg_out_ac_OCTH}')\n",
    "'''\n",
    "print(f'Average in-sample accuracy for XGBoost: {avg_in_ac_XGBoost}')\n",
    "print(f'Average out-sample accuracy for XGBoost: {avg_out_ac_XGBoost}')\n",
    "\n",
    "print(f'Average in-sample accuracy for Random Forest: {avg_in_ac_RF}')\n",
    "print(f'Average out-sample accuracy for Random Forest: {avg_out_ac_RF}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
